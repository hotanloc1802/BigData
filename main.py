import asyncio
import os
import json
import argparse
from src.data_collection import fetch_tiktok_comments
from src.data_cleaning import clean_comment_text
from src.embedding_utils import FastTextEmbedder, CharBiLSTMEmbedder, XLMREmbedder, get_fused_embeddings_and_iob2_labels
from src.video_processor import process_video_pipeline 
from src.data_fusion import fuse_data_pipelines # Import h√†m g·ªôp d·ªØ li·ªáu

# --- Kh·ªüi t·∫°o Embedder (kh·ªüi t·∫°o m·ªôt l·∫ßn ƒë·ªÉ t√°i s·ª≠ d·ª•ng) ---
# C√°c instance n√†y ch·ªâ d√πng cho pipeline b√¨nh lu·∫≠n (comments pipeline)
fasttext_embedder_instance = None
char_bilstm_embedder_instance = None
xlmr_embedder_instance = None 

def initialize_embedders():
    """Kh·ªüi t·∫°o c√°c embedder n·∫øu ch√∫ng ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. C√°c embedder n√†y ch·ªâ d√πng cho pipeline b√¨nh lu·∫≠n."""
    global fasttext_embedder_instance, char_bilstm_embedder_instance, xlmr_embedder_instance
    if fasttext_embedder_instance is None: # Ch·ªâ kh·ªüi t·∫°o n·∫øu ch∆∞a c√≥
        try:
            fasttext_embedder_instance = FastTextEmbedder()
            char_bilstm_embedder_instance = CharBiLSTMEmbedder()
            xlmr_embedder_instance = XLMREmbedder() 
            print("‚úÖ ƒê√£ kh·ªüi t·∫°o t·∫•t c·∫£ Embedder cho pipeline b√¨nh lu·∫≠n.")
        except RuntimeError as e:
            print(f"‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o m·ªôt ho·∫∑c nhi·ªÅu Embedder: {e}")
            print("Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n m√¥ h√¨nh, k·∫øt n·ªëi internet v√† c√†i ƒë·∫∑t th∆∞ vi·ªán.")
            raise 

async def run_comments_pipeline(args, ms_token, video_url_for_comments):
    """Ch·∫°y to√†n b·ªô pipeline x·ª≠ l√Ω b√¨nh lu·∫≠n."""
    # ƒê√£ gi·ªØ nguy√™n ƒë∆∞·ªùng d·∫´n b·∫°n cung c·∫•p
    raw_comments_dir = "data/comment/raw"
    raw_comments_filename = "video_comments_raw.json"
    raw_comments_filepath = os.path.join(raw_comments_dir, raw_comments_filename)

    # ƒê√£ gi·ªØ nguy√™n ƒë∆∞·ªùng d·∫´n b·∫°n cung c·∫•p
    cleaned_comments_dir = "data/comment/cleaned"
    cleaned_comments_filename = "comments_formatted_for_labeling.json" 
    cleaned_comments_filepath = os.path.join(cleaned_comments_dir, cleaned_comments_filename)

    # ƒê√£ gi·ªØ nguy√™n ƒë∆∞·ªùng d·∫´n b·∫°n cung c·∫•p
    preprocessed_comments_dir = "data/comment/preprocessed"
    preprocessed_comments_filename = "comments_with_embeddings_and_labels.jsonl"
    preprocessed_comments_filepath = os.path.join(preprocessed_comments_dir, preprocessed_comments_filename)

    print("\n===== B·∫Øt ƒë·∫ßu Pipeline X·ª≠ l√Ω B√¨nh lu·∫≠n =====")
    
    # B∆∞·ªõc 1: Thu th·∫≠p d·ªØ li·ªáu th√¥
    if args.comment_step in ["full", "collect"]:
        print(f"\n--- B·∫Øt ƒë·∫ßu qu√° tr√¨nh thu th·∫≠p b√¨nh lu·∫≠n cho video: {video_url_for_comments} ---")
        await fetch_tiktok_comments(ms_token, video_url_for_comments, raw_comments_filepath, count=50)
        print("--- Qu√° tr√¨nh thu th·∫≠p b√¨nh lu·∫≠n th√¥ ho√†n t·∫•t ---")
        if args.comment_step == "collect":
            print("\n--- Ch·ªâ ch·∫°y b∆∞·ªõc thu th·∫≠p b√¨nh lu·∫≠n. Ho√†n t·∫•t pipeline b√¨nh lu·∫≠n. ---")
            return 

    # B∆∞·ªõc 2: L√†m s·∫°ch d·ªØ li·ªáu
    if args.comment_step in ["full", "clean"]:
        print("\n--- B·∫Øt ƒë·∫ßu qu√° tr√¨nh l√†m s·∫°ch d·ªØ li·ªáu b√¨nh lu·∫≠n ---")
        if not os.path.exists(cleaned_comments_dir):
            os.makedirs(cleaned_comments_dir)
            print(f"‚úÖ ƒê√£ t·∫°o th∆∞ m·ª•c: {cleaned_comments_dir}")

        formatted_cleaned_comments_list = []

        if os.path.exists(raw_comments_filepath):
            try:
                with open(raw_comments_filepath, "r", encoding="utf-8") as f:
                    raw_data = json.load(f)
                
                total_processed_count = 0
                for comment_entry in raw_data.get("comments", []):
                    original_text = comment_entry.get("text", "")
                    cleaned_text = clean_comment_text(original_text)
                    
                    total_processed_count += 1

                    if cleaned_text: 
                        formatted_comment = {
                            "text": cleaned_text,
                            "labels": [] 
                        }
                        # ƒê·∫£m b·∫£o video_id v√† video_url ƒë∆∞·ª£c b·∫£o to√†n sau b∆∞·ªõc l√†m s·∫°ch n·∫øu ch√∫ng c√≥ trong raw_data
                        if "video_id" in comment_entry: formatted_comment["video_id"] = comment_entry["video_id"]
                        if "video_url" in comment_entry: formatted_comment["video_url"] = comment_entry["video_url"]

                        formatted_cleaned_comments_list.append(formatted_comment)
                    else:
                        print(f"  ‚ö†Ô∏è B·ªè qua b√¨nh lu·∫≠n (ID g·ªëc: {comment_entry.get('id')}) v√¨ qu√° ng·∫Øn sau khi l√†m s·∫°ch.")
                
                with open(cleaned_comments_filepath, "w", encoding="utf-8") as f:
                    json.dump(formatted_cleaned_comments_list, f, indent=2, ensure_ascii=False)
                
                print(f"‚úÖ Ho√†n t·∫•t l√†m s·∫°ch v√† ƒë·ªãnh d·∫°ng! ƒê√£ l∆∞u {len(formatted_cleaned_comments_list)} b√¨nh lu·∫≠n v√†o {cleaned_comments_filepath} theo ƒë·ªãnh d·∫°ng s·∫µn s√†ng g√°n nh√£n.")
                print(f"üí° B∆∞·ªõc ti·∫øp theo: Vui l√≤ng g√°n nh√£n th·ªß c√¥ng cho d·ªØ li·ªáu trong '{cleaned_comments_filepath}' v√† l∆∞u n√≥ v√†o th∆∞ m·ª•c 'data/labeled/' (v√≠ d·ª•: 'data/labeled/your_manually_labeled_data.json').")

            except json.JSONDecodeError as e:
                print(f"‚ùå L·ªói ƒë·ªçc file JSON th√¥ {raw_comments_filepath}: {e}")
            except Exception as e:
                print(f"‚ùå L·ªói trong qu√° tr√¨nh l√†m s·∫°ch v√† ƒë·ªãnh d·∫°ng d·ªØ li·ªáu: {e}")
        else:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu th√¥: {raw_comments_filepath}. Vui l√≤ng ƒë·∫£m b·∫£o b∆∞·ªõc thu th·∫≠p d·ªØ li·ªáu th√†nh c√¥ng tr∆∞·ªõc khi ch·∫°y b∆∞·ªõc l√†m s·∫°ch.")
        
        if args.comment_step == "clean":
            print("\n--- Ch·ªâ ch·∫°y b∆∞·ªõc l√†m s·∫°ch b√¨nh lu·∫≠n. Ho√†n t·∫•t pipeline b√¨nh lu·∫≠n. ---")
            return 

    # B∆∞·ªõc 3: T·∫°o Embeddings v√† IOB2 Labels (Y√™u c·∫ßu d·ªØ li·ªáu ƒë√£ g√°n nh√£n)
    if args.comment_step in ["full", "embed"]:
        print("\n--- B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o Embeddings v√† IOB2 Labels cho b√¨nh lu·∫≠n ---")
        
        # Kh·ªüi t·∫°o c√°c embedder n·∫øu ch∆∞a c√≥
        if fasttext_embedder_instance is None: 
            try:
                initialize_embedders()
            except RuntimeError:
                print("B·ªè qua b∆∞·ªõc t·∫°o embeddings do kh√¥ng th·ªÉ kh·ªüi t·∫°o Embedder.")
                return 

        # ƒê√£ gi·ªØ nguy√™n ƒë∆∞·ªùng d·∫´n b·∫°n cung c·∫•p (gi·∫£ ƒë·ªãnh file n√†y ƒë√£ ƒë∆∞·ª£c g√°n nh√£n th·ªß c√¥ng)
        labeled_data_filepath = r"data\comment\labeled\video_comments_KOL1_1_spans.json" 
        
        if not os.path.exists(preprocessed_comments_dir):
            os.makedirs(preprocessed_comments_dir)
            print(f"‚úÖ ƒê√£ t·∫°o th∆∞ m·ª•c: {preprocessed_comments_dir}")

        labeled_input_data = []
        if os.path.exists(labeled_data_filepath):
            try:
                with open(labeled_data_filepath, 'r', encoding="utf-8") as f:
                    labeled_input_data = json.load(f)
                print(f"‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu ƒë√£ g√°n nh√£n t·ª´: {labeled_data_filepath} ({len(labeled_input_data)} m·∫´u)")
                
                processed_samples = []
                for i, sample in enumerate(labeled_input_data):
                    print(f"\n--- X·ª≠ l√Ω m·∫´u {i+1}/{len(labeled_input_data)}: {sample['text'][:50]}... ---")
                    
                    # Truy·ªÅn video_id v√† video_url t·ª´ sample ƒë√£ g√°n nh√£n n·∫øu c√≥
                    temp_video_id = sample.get("video_id")
                    temp_video_url = sample.get("video_url")

                    fused_embs, iob2_lbls, tokens = get_fused_embeddings_and_iob2_labels(
                        sample, fasttext_embedder_instance, char_bilstm_embedder_instance, xlmr_embedder_instance
                    )

                    if fused_embs is None:
                        print(f"B·ªè qua m·∫´u {i+1} do l·ªói trong qu√° tr√¨nh t·∫°o embeddings/nh√£n ho·∫∑c kh√¥ng c√≥ token.")
                        continue

                    processed_comment = {
                        "video_id": "7519112491503226119",
                        "original_text": sample['text'],
                        "word_tokens": tokens,
                        "iob2_labels": iob2_lbls,
                        "embeddings": fused_embs.tolist()
                    }
                    if temp_video_id: processed_comment["video_id"] = temp_video_id
                    if temp_video_url: processed_comment["video_url"] = temp_video_url
                    
                    processed_samples.append(processed_comment)
                
                with open(preprocessed_comments_filepath, 'w', encoding="utf-8") as f:
                    for entry in processed_samples:
                        f.write(json.dumps(entry, ensure_ascii=False) + '\n')

                print(f"\n‚úÖ K·∫øt qu·∫£ Embeddings v√† IOB2 Labels ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {preprocessed_comments_filepath}")
                print("\n--- Ho√†n t·∫•t qu√° tr√¨nh t·∫°o Fused Embeddings v√† IOB2 Labels cho b√¨nh lu·∫≠n ---")

            except json.JSONDecodeError as e:
                print(f"‚ùå L·ªói ƒë·ªçc file JSON ƒë√£ g√°n nh√£n {labeled_data_filepath}: {e}")
            except Exception as e:
                print(f"‚ùå L·ªói trong qu√° tr√¨nh t·∫°o Embeddings v√† IOB2 Labels: {e}")
        else:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu ƒë√£ g√°n nh√£n: {labeled_data_filepath}.")
            print("Vui l√≤ng g√°n nh√£n th·ªß c√¥ng cho d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch v√† l∆∞u v√†o ƒë∆∞·ªùng d·∫´n n√†y ƒë·ªÉ ti·∫øp t·ª•c b∆∞·ªõc t·∫°o embeddings.")
    
    print("\n===== Ho√†n t·∫•t Pipeline X·ª≠ l√Ω B√¨nh lu·∫≠n =====")


async def run_videos_pipeline(video_url_for_videos): 
    """Ch·∫°y to√†n b·ªô pipeline x·ª≠ l√Ω video."""
    # ƒê√£ gi·ªØ nguy√™n ƒë∆∞·ªùng d·∫´n b·∫°n cung c·∫•p
    video_raw_dir = "data/script/raw" 
    video_summary_dir = "data/script/summarized" 

    print("\n===== B·∫Øt ƒë·∫ßu Pipeline X·ª≠ l√Ω Video =====")
    await process_video_pipeline(video_url_for_videos, video_raw_dir, video_summary_dir)
    print("\n===== Ho√†n t·∫•t Pipeline X·ª≠ l√Ω Video =====")


async def main():
    parser = argparse.ArgumentParser(description="Pipeline x·ª≠ l√Ω b√¨nh lu·∫≠n v√† video TikTok.")
    parser.add_argument(
        "--process", 
        type=str, 
        default="full", 
        choices=["full", "comments", "videos", "fuse"], # Th√™m l·ª±a ch·ªçn 'fuse'
        help="Ch·ªçn quy tr√¨nh x·ª≠ l√Ω: 'full' (c·∫£ b√¨nh lu·∫≠n v√† video), 'comments' (ch·ªâ b√¨nh lu·∫≠n), 'videos' (ch·ªâ video), 'fuse' (ch·ªâ g·ªôp d·ªØ li·ªáu)."
    )
    parser.add_argument(
        "--comment_step", 
        type=str, 
        default="full", 
        choices=["full", "collect", "clean", "embed"],
        help="Ch·ªçn b∆∞·ªõc x·ª≠ l√Ω b√¨nh lu·∫≠n: 'full' (m·∫∑c ƒë·ªãnh), 'collect' (ch·ªâ thu th·∫≠p), 'clean' (ch·ªâ l√†m s·∫°ch), 'embed' (ch·ªâ t·∫°o embeddings)."
    )
    args = parser.parse_args()

    # --- C·∫•u h√¨nh chung ---
    ms_token = "mgsvpemIp8ZglZO-QXrDRhSwIVqO3kI7wf5k9EHE80rP-KpO8-FwK2u9d4yommFAM5FJbxZLdNIGzftBRBq8mzbCUuHSfnph24FbgxhZwtkOCs3XtUTLEuq5u6qF5O9TSmWEyxVXnrEquZbRFRCmALDz"
    video_url_for_comments = "https://www.tiktok.com/@halinhofficial/video/7519112491503226119" 
    video_url_for_videos = "https://www.tiktok.com/@chouchinchan/video/7519112491503226119"

    # --- ƒê·ªãnh nghƒ©a c√°c ƒë∆∞·ªùng d·∫´n cho b∆∞·ªõc G·ªôp d·ªØ li·ªáu ---
    # ƒê√£ s·ª≠a l·ªói: Gi·ªù l√† ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·∫øn file JSONL
    comments_embedded_filepath = os.path.join("data", "comment", "preprocessed", "comments_with_embeddings_and_labels.jsonl")
    
    # ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a c√°c file JSON c·ªßa script video ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† s·∫µn s√†ng g√°n nh√£n
    # (output t·ª´ video_processor.py)
    # ƒê√£ gi·ªØ nguy√™n ƒë∆∞·ªùng d·∫´n b·∫°n cung c·∫•p
    video_scripts_for_fusion_dir = os.path.join("data", "script", "preprocessed") # <-- D·ªØ li·ªáu script nh√∫ng s·∫Ω ·ªü ƒë√¢y

    # ƒê∆∞·ªùng d·∫´n file ƒë·∫ßu ra sau khi g·ªôp
    # ƒê√£ s·ª≠a l·ªói: Gi·ªù l√† ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·∫øn file JSON
    output_fused_filepath = os.path.join("data", "combined", "fused_video_comment_features.json")
    
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c 'combined' t·ªìn t·∫°i
    os.makedirs(os.path.dirname(output_fused_filepath), exist_ok=True)


    # --- Ch·∫°y c√°c Pipeline ---
    if args.process == "full":
        print("\n--- B·∫Øt ƒë·∫ßu ch·∫°y song song Pipeline X·ª≠ l√Ω B√¨nh lu·∫≠n v√† Video ---")
        try:
            # Kh·ªüi t·∫°o embedder cho pipeline b√¨nh lu·∫≠n (ch·ªâ khi c·∫ßn)
            # Pipeline video t·ª± qu·∫£n l√Ω vi·ªác t·∫£i model c·ªßa n√≥
            await asyncio.gather(
                run_comments_pipeline(args, ms_token, video_url_for_comments),
                run_videos_pipeline(video_url_for_videos) 
            )
        except RuntimeError: 
            print("M·ªôt trong c√°c pipeline ƒë√£ g·∫∑p l·ªói kh·ªüi t·∫°o. D·ª´ng to√†n b·ªô qu√° tr√¨nh.")
            return
        
        # Sau khi c·∫£ hai pipeline ho√†n th√†nh, ch·∫°y b∆∞·ªõc g·ªôp d·ªØ li·ªáu
        print("\n--- B·∫Øt ƒë·∫ßu b∆∞·ªõc g·ªôp d·ªØ li·ªáu sau khi c√°c pipeline ch√≠nh ho√†n th√†nh ---")
        fuse_data_pipelines(comments_embedded_filepath, video_scripts_for_fusion_dir, output_fused_filepath)


    elif args.process == "comments":
        print("\n--- B·∫Øt ƒë·∫ßu ch·∫°y Pipeline X·ª≠ l√Ω B√¨nh lu·∫≠n ---")
        try:
            await run_comments_pipeline(args, ms_token, video_url_for_comments)
        except RuntimeError:
            print("Pipeline b√¨nh lu·∫≠n ƒë√£ g·∫∑p l·ªói kh·ªüi t·∫°o. D·ª´ng qu√° tr√¨nh.")
            return

    elif args.process == "videos":
        print("\n--- B·∫Øt ƒë·∫ßu ch·∫°y Pipeline X·ª≠ l√Ω Video ---")
        await run_videos_pipeline(video_url_for_videos) 

    elif args.process == "fuse": # L·ª±a ch·ªçn m·ªõi ƒë·ªÉ ch·ªâ ch·∫°y b∆∞·ªõc g·ªôp
        print("\n--- B·∫Øt ƒë·∫ßu ch·∫°y b∆∞·ªõc G·ªôp d·ªØ li·ªáu (Data Fusion) ---")
        fuse_data_pipelines(comments_embedded_filepath, video_scripts_for_fusion_dir, output_fused_filepath)


    print("\n--- To√†n b·ªô c√°c Pipeline x·ª≠ l√Ω d·ªØ li·ªáu ƒë√£ ho√†n t·∫•t ---")

if __name__ == "__main__":
    asyncio.run(main())