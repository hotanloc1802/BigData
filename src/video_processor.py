import subprocess
import os
import whisper
import time
import json
from datetime import datetime
from src.data_cleaning import clean_comment_text
import torch 
import torch.nn as nn 
import fasttext 
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel 
from pyvi import ViTokenizer 

# --- B·∫Øt ƒë·∫ßu ph·∫ßn code tr√πng l·∫∑p ƒë·ªÉ t·∫°o Fused Embedding cho video script ---
# V√å Y√äU C·∫¶U "K THAY ƒê·ªîI EMBEDDING_UTILIS" V√Ä "K S·ª¨ D·ª§NG EMBEDDING_UTILIS TRONG VIDEO_PROCESSOR"
# ƒê√¢y l√† c√°c h·∫±ng s·ªë v√† l·ªõp/h√†m ƒë∆∞·ª£c sao ch√©p/t√°i t·∫°o t·ª´ src/embedding_utils.py ƒë·ªÉ s·ª≠ d·ª•ng ri√™ng cho video_processor
# ƒë·ªÉ c√≥ ƒë∆∞·ª£c fused embedding 1168 chi·ªÅu.

# Tham s·ªë c·∫•u h√¨nh cho c√°c Embedder trong video_processor
FASTTEXT_MODEL_PATH = os.path.join("models", "fasttext", "cc.vi.300.bin") 
FASTTEXT_EMBEDDING_DIM = 300

CHAR_EMBEDDING_DIM = 50
CHAR_LSTM_HIDDEN_DIM = 50
CHAR_FEATURE_OUTPUT_DIM = CHAR_LSTM_HIDDEN_DIM * 2

# ƒê√£ s·ª≠a l·∫°i ƒë∆∞·ªùng d·∫´n c·ªßa m√¥ h√¨nh XLM-RoBERTa ƒë·ªÉ tr·ªè ƒë·∫øn th∆∞ m·ª•c c·ª•c b·ªô (ƒê√£ s·ª≠ d·ª•ng os.path.join)
XLMR_MODEL_NAME = os.path.join("models", "huggingface", "hub", "models--xlm-roberta-base", "snapshots", "e73636d4f797dec63c3081bb6ed5c7b0bb3f2089")
XLMR_EMBEDDING_DIM = 768

# C·∫•u h√¨nh m√¥ h√¨nh t√≥m t·∫Øt (ƒê√£ s·ª≠a l·∫°i t√™n m√¥ h√¨nh v√† s·ª≠ d·ª•ng os.path.join)
SUMMARIZATION_MODEL_NAME = os.path.join("models", "huggingface", "hub", "models--VietAI--vit5-large-vietnews-summarization", "snapshots", "8926b4edb541fd4f28260e5469d8a00121785fcf")
summarization_tokenizer = None
summarization_model = None


# √Ånh x·∫° k√Ω t·ª± sang ch·ªâ s·ªë (C·∫ßn thi·∫øt cho CharBiLSTMEmbedder)
# ƒê√£ s·ª≠a l·ªói k√Ω t·ª± kh√¥ng in ƒë∆∞·ª£c U+00A0 v√† d·∫•u nh√°y ƒë∆°n
video_char_to_idx = {char: i for i, char in enumerate(
    'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789√°√†·∫£·∫°√£ƒÉ·∫±·∫≥·∫µ·∫∑√¢·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫π·∫Ω√™·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâ·ªãƒ©√≥√≤·ªè·ªç√µ√¥·ªì·ªï·ªó·ªô∆°·ªù·ªü·ª°·ª£√∫√π·ªß·ª•≈©∆∞·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªµ·ªπƒëƒê,.:;?!\'\"()[]{}<>/-_&@#$%^&*=+~`'
)}
video_char_to_idx["<PAD>"] = 0
video_char_to_idx["<unk>"] = len(video_char_to_idx)
VIDEO_CHAR_VOCAB_SIZE = len(video_char_to_idx)

class VideoFastTextEmbedder:
    def __init__(self, model_path=FASTTEXT_MODEL_PATH):
        try:
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh FastText cho video t·∫°i: {model_path}")
            self.model = fasttext.load_model(model_path)
            self.embedding_dim = self.model.get_word_vector("test").shape[0]
            print(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh FastText cho video t·ª´: {model_path} (dim: {self.embedding_dim})")
        except Exception as e:
            raise RuntimeError(f"L·ªói khi t·∫£i m√¥ h√¨nh FastText cho video t·ª´ '{model_path}': {e}.")
    def get_embedding(self, word):
        return torch.from_numpy(self.model.get_word_vector(word)).float()

class VideoCharBiLSTMEmbedder(nn.Module):
    def __init__(self, vocab_size=VIDEO_CHAR_VOCAB_SIZE, embedding_dim=CHAR_EMBEDDING_DIM, hidden_dim=CHAR_LSTM_HIDDEN_DIM):
        super(VideoCharBiLSTMEmbedder, self).__init__()
        self.char_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=video_char_to_idx["<PAD>"])
        self.char_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.output_dim = hidden_dim * 2
    def forward(self, char_indices_list_per_word):
        word_char_representations = []
        for char_indices_for_word in char_indices_list_per_word:
            if char_indices_for_word.numel() == 0:
                word_char_representations.append(torch.zeros(self.output_dim))
                continue
            char_embs = self.char_embedding(char_indices_for_word.unsqueeze(0))
            _, (h_n, _) = self.char_lstm(char_embs)
            char_word_representation = torch.cat((h_n[0], h_n[1]), dim=1).squeeze(0)
            word_char_representations.append(char_word_representation)
        if not word_char_representations:
            return torch.empty(0, self.output_dim)
        return torch.stack(word_char_representations)

class VideoXLMREmbedder: # T√°i s·ª≠ d·ª•ng nh∆∞ng v·ªõi t√™n kh√°c ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n
    def __init__(self, model_name=XLMR_MODEL_NAME):
        try:
            if not os.path.exists(model_name) or \
               not (os.path.exists(os.path.join(model_name, 'tokenizer.json')) or \
                    os.path.exists(os.path.join(model_name, 'config.json'))):
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh XLM-RoBERta cho video t·∫°i: {model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name)
            self.model.eval()
            self.embedding_dim = self.model.config.hidden_size
            print(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh XLM-RoBERTa cho video t·ª´: {model_name} (dim: {self.embedding_dim})")
        except Exception as e:
            raise RuntimeError(f"L·ªói khi t·∫£i m√¥ h√¨nh XLM-RoBERTa cho video t·ª´ '{model_name}': {e}.")

    def get_token_embeddings(self, text):
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512, return_offsets_mapping=True)
        offset_mapping = inputs.pop('offset_mapping')
        with torch.no_grad():
            outputs = self.model(**inputs)
            token_embeddings = outputs.last_hidden_state.squeeze(0)
        return token_embeddings, inputs['input_ids'].squeeze(0), offset_mapping.squeeze(0)

# C√°c instance global cho embedder trong video_processor
video_fasttext_embedder_instance = None
video_char_bilstm_embedder_instance = None
video_xlm_roberta_embedder_instance = None # Instance ri√™ng cho video_processor

def load_video_script_embedders():
    """T·∫£i v√† kh·ªüi t·∫°o t·∫•t c·∫£ c√°c embedder c·∫ßn thi·∫øt cho video script."""
    global video_fasttext_embedder_instance, video_char_bilstm_embedder_instance, video_xlm_roberta_embedder_instance
    if video_fasttext_embedder_instance is None:
        try:
            video_fasttext_embedder_instance = VideoFastTextEmbedder()
            video_char_bilstm_embedder_instance = VideoCharBiLSTMEmbedder()
            video_xlm_roberta_embedder_instance = VideoXLMREmbedder()
            print("‚úÖ ƒê√£ kh·ªüi t·∫°o t·∫•t c·∫£ Embedder cho video script.")
        except RuntimeError as e:
            print(f"‚ùå L·ªói khi kh·ªüi t·∫°o Embedder cho video script: {e}")
            raise # B√°o l·ªói ƒë·ªÉ d·ª´ng n·∫øu kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh

def process_text_to_word_tokens_for_video(text): # Sao ch√©p t·ª´ embedding_utils/text_utils
    """T√°ch vƒÉn b·∫£n th√†nh c√°c word/syllable tokens b·∫±ng pyvi."""
    cleaned_text = text # Gi·∫£ ƒë·ªãnh ƒë√£ clean b·ªüi clean_text_content
    tokens_str = ViTokenizer.tokenize(cleaned_text)
    tokens = tokens_str.replace("_", " ").split()
    # (ƒê·ªÉ ƒë∆°n gi·∫£n, b·ªè qua logic cƒÉn ch·ªânh ph·ª©c t·∫°p n·∫øu n√≥ kh√¥ng c·∫ßn cho fused sentence embedding)
    return tokens, cleaned_text, None # Tr·∫£ v·ªÅ None cho token_info_list n·∫øu kh√¥ng d√πng

def get_char_indices_for_words_for_video(cleaned_text, tokens, char_to_idx): # Sao ch√©p t·ª´ embedding_utils/text_utils
    """T·∫°o danh s√°ch c√°c tensor ch·ªâ s·ªë k√Ω t·ª± cho m·ªói t·ª´/√¢m ti·∫øt."""
    char_indices_list_per_word = []
    for token in tokens:
        word_char_indices = [char_to_idx.get(c, char_to_idx['<unk>']) for c in token] # D√πng token tr·ª±c ti·∫øp
        char_indices_list_per_word.append(torch.tensor(word_char_indices, dtype=torch.long))
    return char_indices_list_per_word

def get_fused_sentence_embedding_for_script(script_text: str) -> list:
    """
    T·∫°o fused sentence embedding (1168 chi·ªÅu) cho script.
    """
    if video_fasttext_embedder_instance is None: # ƒê·∫£m b·∫£o ƒë√£ t·∫£i c√°c embedder
        load_video_script_embedders()

    # B∆∞·ªõc 1: Tokenize vƒÉn b·∫£n
    tokens, _, _ = process_text_to_word_tokens_for_video(script_text)
    if not tokens:
        return torch.zeros(FASTTEXT_EMBEDDING_DIM + CHAR_FEATURE_OUTPUT_DIM + XLMR_EMBEDDING_DIM).tolist() # S·ª≠ d·ª•ng h·∫±ng s·ªë chung

    # B∆∞·ªõc 2: T·∫°o FastText Embeddings v√† Mean Pooling
    fasttext_word_embs = torch.stack([video_fasttext_embedder_instance.get_embedding(token) for token in tokens])
    fasttext_sentence_emb = torch.mean(fasttext_word_embs, dim=0)

    # B∆∞·ªõc 3: T·∫°o Character Embeddings v√† Mean Pooling
    char_indices_list_per_word = get_char_indices_for_words_for_video(script_text, tokens, video_char_to_idx)
    character_word_embs = video_char_bilstm_embedder_instance(char_indices_list_per_word)
    character_sentence_emb = torch.mean(character_word_embs, dim=0)

    # B∆∞·ªõc 4: T·∫°o XLM-RoBERTa Sentence Embedding (d√πng CLS token)
    # T√°i s·ª≠ d·ª•ng get_sentence_embedding_from_xlmr ƒë√£ c√≥
    xlmr_sentence_emb = torch.tensor(get_sentence_embedding_from_xlmr_internal(script_text))


    # B∆∞·ªõc 5: N·ªëi c√°c embedding
    fused_embedding = torch.cat(
        (fasttext_sentence_emb, character_sentence_emb, xlmr_sentence_emb), dim=0
    )
    return fused_embedding.tolist()

def get_sentence_embedding_from_xlmr_internal(text: str) -> list:
    """
    T·∫°o sentence embedding t·ª´ vƒÉn b·∫£n b·∫±ng m√¥ h√¨nh XLM-RoBERTa ƒë∆∞·ª£c t·∫£i c·ª•c b·ªô (d√πng trong video_processor).
    """
    if video_xlm_roberta_embedder_instance is None:
        load_video_script_embedders() # T·∫£i t·∫•t c·∫£ embedder n·∫øu ch∆∞a c√≥
    
    inputs = video_xlm_roberta_embedder_instance.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = video_xlm_roberta_embedder_instance.model(**inputs)
        sentence_embedding = outputs.last_hidden_state[0, 0, :] 
    return sentence_embedding.tolist()

# --- C√°c h√†m kh√°c (download_video, extract_audio, transcribe_audio, summarize_transcript) gi·ªØ nguy√™n ---

# üîß ƒê∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi ƒë·∫øn ffmpeg (c·∫ßn ƒë·∫£m b·∫£o ffmpeg c√≥ trong PATH ho·∫∑c cung c·∫•p ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß)
FFMPEG_PATH = "ffmpeg" 


summarization_tokenizer = None
summarization_model = None


def load_summarization_model():
    """T·∫£i tokenizer v√† m√¥ h√¨nh t√≥m t·∫Øt n·∫øu ch√∫ng ch∆∞a ƒë∆∞·ª£c t·∫£i."""
    global summarization_tokenizer, summarization_model
    if summarization_tokenizer is None:
        print(f"‚ú® ƒêang t·∫£i m√¥ h√¨nh t√≥m t·∫Øt: {SUMMARIZATION_MODEL_NAME}...")
        try:
            tokenizer_loaded = AutoTokenizer.from_pretrained(SUMMARIZATION_MODEL_NAME)
            model_loaded = AutoModelForSeq2SeqLM.from_pretrained(SUMMARIZATION_MODEL_NAME)
            
            if not callable(tokenizer_loaded): 
                raise TypeError("ƒê·ªëi t∆∞·ª£ng tokenizer ƒë∆∞·ª£c t·∫£i kh√¥ng ph·∫£i l√† m·ªôt h√†m (callable).")

            summarization_tokenizer = tokenizer_loaded
            summarization_model = model_loaded
            summarization_model.eval() 
            print(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh t√≥m t·∫Øt t·ª´: {SUMMARIZATION_MODEL_NAME}")
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh t√≥m t·∫Øt '{SUMMARIZATION_MODEL_NAME}': {e}")
            summarization_tokenizer = None 
            summarization_model = None 
            raise RuntimeError(f"Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh t√≥m t·∫Øt.") 

def get_video_info(url):
    """L·∫•y th√¥ng tin JSON c·ªßa video t·ª´ URL b·∫±ng yt-dlp."""
    try:
        cmd = ["yt-dlp", "--print-json", url]
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        video_info = json.loads(result.stdout)
        return video_info
    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        print(f"‚ùå L·ªói khi l·∫•y th√¥ng tin video b·∫±ng yt-dlp: {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"‚ùå L·ªói gi·∫£i m√£ JSON t·ª´ yt-dlp: {e}")
        return None

def download_video(url, output_file_path):
    """T·∫£i video t·ª´ URL b·∫±ng yt-dlp."""
    try:
        print(f"üì• ƒêang t·∫£i video: {url}...")
        cmd = ["yt-dlp", "-f", "bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4", "-o", output_file_path, url] 
        subprocess.run(cmd, check=True, capture_output=True, text=True)
        print(f"‚úÖ ƒê√£ t·∫£i video: {output_file_path}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå L·ªói yt-dlp:\nSTDOUT: {e.stdout}\nSTDERR: {e.stderr}")
        return False
    except FileNotFoundError:
        print("‚ùå L·ªói: yt-dlp kh√¥ng t√¨m th·∫•y. Vui l√≤ng ƒë·∫£m b·∫£o yt-dlp ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t v√† c√≥ trong PATH.")
        return False
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i video: {e}")
        return False

def extract_audio(input_video_path, output_audio_path):
    """T√°ch √¢m thanh t·ª´ video v√† chuy·ªÉn ƒë·ªïi sang ƒë·ªãnh d·∫°ng WAV (16kHz, mono) b·∫±ng FFmpeg."""
    try:
        print(f"üéß ƒêang t√°ch √¢m thanh t·ª´ {input_video_path} (chu·∫©n .wav)...")
        cmd = [
            FFMPEG_PATH,
            "-i", input_video_path,
            "-vn", 
            "-acodec", "pcm_s16le", 
            "-ar", "16000",  
            "-ac", "1",      
            "-y", 
            output_audio_path
        ]
        subprocess.run(cmd, check=True, capture_output=True, text=True)
        print(f"‚úÖ √Çm thanh ƒë√£ l∆∞u: {output_audio_path}")
        return output_audio_path
    except subprocess.CalledProcessError as e:
        print(f"‚ùå FFmpeg l·ªói:\nSTDOUT: {e.stdout}\nSTDERR: {e.stderr}")
        print("Vui l√≤ng ƒë·∫£m b·∫£o FFmpeg ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t v√† c√≥ trong PATH h·ªá th·ªëng.")
        return None
    except FileNotFoundError:
        print("‚ùå L·ªói: FFmpeg kh√¥ng t√¨m th·∫•y. Vui l√≤ng ƒë·∫£m b·∫£o FFmpeg ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t v√† c√≥ trong PATH.")
        return None
    except Exception as e:
        print(f"‚ùå L·ªói khi t√°ch √¢m thanh: {e}")
        return None

def transcribe_audio(audio_file_path, whisper_model_name="medium", download_root="models/whisper"):
    """Th·ª±c hi·ªán chuy·ªÉn ƒë·ªïi gi·ªçng n√≥i th√†nh vƒÉn b·∫£n b·∫±ng m√¥ h√¨nh Whisper."""
    try:
        print(f"üß† ƒêang nh·∫≠n d·∫°ng v·ªõi Whisper ({whisper_model_name}): {audio_file_path}")
        if not os.path.exists(audio_file_path):
            print("‚ùå File √¢m thanh kh√¥ng t·ªìn t·∫°i.")
            return ""
        
        if not os.path.exists(download_root):
            os.makedirs(download_root)
            print(f"‚úÖ ƒê√£ t·∫°o th∆∞ m·ª•c Whisper model: {download_root}")

        model = whisper.load_model(whisper_model_name, download_root=download_root)
        result = model.transcribe(audio_file_path, language="vi")
        print("üìù Transcript:", result["text"][:200], "...") 
        return result["text"]
    except Exception as e:
        print(f"‚ùå L·ªói khi nh·∫≠n d·∫°ng v·ªõi Whisper: {e}")
        import traceback
        traceback.print_exc()
        return ""

async def summarize_transcript(transcript_text: str) -> str:
    """
    T√≥m t·∫Øt n·ªôi dung c·ªßa transcript s·ª≠ d·ª•ng m√¥ h√¨nh VietAI/vit5-large-vietnews-summarization.
    """
    if not transcript_text:
        return "Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ t√≥m t·∫Øt."

    try:
        if summarization_model is None or summarization_tokenizer is None:
            load_summarization_model()

        print(f"\n‚ú® ƒêang t√≥m t·∫Øt transcript b·∫±ng m√¥ h√¨nh: {SUMMARIZATION_MODEL_NAME}...")
        
        input_ids = summarization_tokenizer(transcript_text, return_tensors="pt", max_length=512, truncation=True).input_ids
        
        with torch.no_grad(): 
            generated_ids = summarization_model.generate(
                input_ids, 
                max_length=150, 
                min_length=30, 
                num_beams=5, 
                early_stopping=True
            )
        summary = summarization_tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        
        print(f"‚úÖ ƒê√£ t√≥m t·∫Øt: {summary}")
        return summary
    except Exception as e:
        print(f"‚ùå L·ªói khi t√≥m t·∫Øt b·∫±ng m√¥ h√¨nh: {e}")
        import traceback
        traceback.print_exc()
        return "Kh√¥ng th·ªÉ t√≥m t·∫Øt do l·ªói."

async def process_video_pipeline(video_url: str, output_raw_dir: str, output_summary_dir: str): 
    """
    Pipeline x·ª≠ l√Ω video: t·∫£i xu·ªëng, tr√≠ch xu·∫•t √¢m thanh, chuy·ªÉn ƒë·ªïi gi·ªçng n√≥i th√†nh vƒÉn b·∫£n, l√†m s·∫°ch, t√≥m t·∫Øt,
    v√† t·∫°o d·ªØ li·ªáu c√≥ c·∫•u tr√∫c ƒë·ªÉ g√°n nh√£n thu·ªôc t√≠nh v·ªõi fused embedding 1168 chi·ªÅu.
    """
    video_id = video_url.split("/")[-1].split("?")[0] 
    
    os.makedirs(output_raw_dir, exist_ok=True)
    os.makedirs(output_summary_dir, exist_ok=True)

    video_filename = os.path.join(output_raw_dir, f"video_{video_id}.mp4")
    audio_filename = os.path.join(output_raw_dir, f"audio_{video_id}.wav")
    raw_transcript_filename = os.path.join(output_raw_dir, f"transcript_raw_{video_id}.txt") 
    cleaned_transcript_filename = os.path.join(output_raw_dir, f"transcript_cleaned_{video_id}.txt") 
    transcript_summary_filepath = os.path.join(output_summary_dir, f"transcript_summary_{video_id}.json")

    # L·∫•y th√¥ng tin video (ti√™u ƒë·ªÅ) tr∆∞·ªõc khi t·∫£i
    video_info = get_video_info(video_url)
    video_title = video_info.get("title", "Unknown Title") if video_info else "Unknown Title"
    print(f"üé• Ti√™u ƒë·ªÅ video: {video_title}")

    # B∆∞·ªõc 1: T·∫£i video
    if not os.path.exists(video_filename):
        if not download_video(video_url, video_filename):
            print("‚ùå D·ª´ng pipeline x·ª≠ l√Ω video do l·ªói t·∫£i video.")
            return
    else:
        print(f"‚úÖ Video ƒë√£ t·ªìn t·∫°i: {video_filename}. B·ªè qua t·∫£i xu·ªëng.")

    # B∆∞·ªõc 2: T√°ch √¢m thanh
    if not os.path.exists(audio_filename):
        audio_path = extract_audio(video_filename, audio_filename)
        if not audio_path:
            print("‚ùå D·ª´ng pipeline x·ª≠ l√Ω video do l·ªói t√°ch √¢m thanh.")
            return
    else:
        print(f"‚úÖ File √¢m thanh ƒë√£ t·ªìn t·∫°i: {audio_filename}. B·ªè qua t√°ch √¢m thanh.")
        audio_path = audio_filename

    # B∆∞·ªõc 3: Nh·∫≠n d·∫°ng gi·ªçng n√≥i (transcript th√¥)
    raw_transcript_text = ""
    if not os.path.exists(raw_transcript_filename):
        raw_transcript_text = transcribe_audio(audio_path)
        if raw_transcript_text:
            with open(raw_transcript_filename, "w", encoding="utf-8") as f:
                f.write(raw_transcript_text)
            print(f"üìÑ Transcript th√¥ ƒë√£ l∆∞u: {raw_transcript_filename}")
        else:
            print("‚ùå Kh√¥ng th·ªÉ t·∫°o transcript th√¥. D·ª´ng pipeline x·ª≠ l√Ω video.")
            return
    else:
        print(f"‚úÖ File transcript th√¥ ƒë√£ t·ªìn t·∫°i: {raw_transcript_filename}. B·ªè qua nh·∫≠n d·∫°ng gi·ªçng n√≥i.")
        with open(raw_transcript_filename, "r", encoding="utf-8") as f:
            raw_transcript_text = f.read()

    # B∆∞·ªõc 4: L√†m s·∫°ch transcript
    cleaned_transcript_text = ""
    if raw_transcript_text and not os.path.exists(cleaned_transcript_filename):
        print("üßπ ƒêang l√†m s·∫°ch transcript...")
        cleaned_transcript_text = clean_comment_text(raw_transcript_text) 
        if cleaned_transcript_text:
            with open(cleaned_transcript_filename, "w", encoding="utf-8") as f:
                f.write(cleaned_transcript_text)
            print(f"‚úÖ Transcript ƒë√£ l√†m s·∫°ch ƒë√£ l∆∞u: {cleaned_transcript_filename}")
        else:
            print("‚ö†Ô∏è Transcript tr·ªëng sau khi l√†m s·∫°ch. B·ªè qua c√°c b∆∞·ªõc ti·∫øp theo.")
            return 
    elif os.path.exists(cleaned_transcript_filename):
        print(f"‚úÖ File transcript ƒë√£ l√†m s·∫°ch ƒë√£ t·ªìn t·∫°i: {cleaned_transcript_filename}. B·ªè qua l√†m s·∫°ch.")
        with open(cleaned_transcript_filename, "r", encoding="utf-8") as f:
            cleaned_transcript_text = f.read()
    else: 
        print("‚ö†Ô∏è Kh√¥ng c√≥ transcript th√¥ ƒë·ªÉ l√†m s·∫°ch.")
        return

    # B∆∞·ªõc 5: T√≥m t·∫Øt transcript (s·ª≠ d·ª•ng m√¥ h√¨nh VietAI/vit5-large-vietnews-summarization)
    # ƒê√£ di chuy·ªÉn l√™n tr∆∞·ªõc ƒë·ªÉ t·∫°o embedding d·ª±a tr√™n t√≥m t·∫Øt
    summary_text = ""
    if cleaned_transcript_text and not os.path.exists(transcript_summary_filepath):
        summary_text = await summarize_transcript(cleaned_transcript_text)
        if summary_text:
            summary_output = {
                "video_id": video_id,
                "video_url": video_url,
                "cleaned_transcript_filepath": cleaned_transcript_filename, 
                "summary": summary_text,
                "timestamp": datetime.now().isoformat()
            }
            with open(transcript_summary_filepath, "w", encoding="utf-8") as f:
                json.dump(summary_output, f, indent=2, ensure_ascii=False)
            print(f"‚úÖ T√≥m t·∫Øt abstractive ƒë√£ l∆∞u: {transcript_summary_filepath}")
        else:
            print("‚ùå Kh√¥ng th·ªÉ t·∫°o t√≥m t·∫Øt abstractive.")
    elif os.path.exists(transcript_summary_filepath):
        print(f"‚úÖ File t√≥m t·∫Øt abstractive ƒë√£ t·ªìn t·∫°i: {transcript_summary_filepath}. B·ªè qua t√≥m t·∫Øt.")
        with open(transcript_summary_filepath, "r", encoding="utf-8") as f:
            summary_data = json.load(f)
            summary_text = summary_data.get("summary", "")
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ transcript ƒë√£ l√†m s·∫°ch ƒë·ªÉ t·∫°o t√≥m t·∫Øt abstractive.")
        return # D·ª´ng pipeline n·∫øu kh√¥ng c√≥ t√≥m t·∫Øt ƒë·ªÉ t·∫°o embedding

    # B∆∞·ªõc 6: T·∫°o script_context_vector (1168 chi·ªÅu) d·ª±a tr√™n T√ìM T·∫ÆT v√† l∆∞u d·ªØ li·ªáu c√≥ c·∫•u tr√∫c ƒë·ªÉ g√°n nh√£n
    script_context_vector = None
    if summary_text: # S·ª≠ d·ª•ng summary_text thay v√¨ cleaned_transcript_text
        print("ÔøΩ ƒêang t·∫°o script context vector (1168 chi·ªÅu) t·ª´ T√ìM T·∫ÆT...")
        try:
            load_video_script_embedders() 
            script_context_vector = get_fused_sentence_embedding_for_script(summary_text) # Thay ƒë·ªïi ·ªü ƒë√¢y
            print(f"‚úÖ ƒê√£ t·∫°o script context vector (dim: {len(script_context_vector)}) t·ª´ t√≥m t·∫Øt.")
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫°o script context vector t·ª´ t√≥m t·∫Øt: {e}")
            script_context_vector = None
    else:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o script context vector (t√≥m t·∫Øt r·ªóng).")

    # L∆∞u d·ªØ li·ªáu c√≥ c·∫•u tr√∫c cho vi·ªác g√°n nh√£n
    structured_output_for_labeling_dir = os.path.join("data", "script", "preprocessed") 
    os.makedirs(structured_output_for_labeling_dir, exist_ok=True)
    structured_output_filepath = os.path.join(structured_output_for_labeling_dir, f"video_script_features_for_labeling_{video_id}.json")

    structured_output = {
        "video_id": video_id,
        "video_url": video_url,
        "video_title": video_title,
        "script_text": cleaned_transcript_text, # V·∫´n gi·ªØ script ƒë·∫ßy ƒë·ªß ƒë·ªÉ tham chi·∫øu
        "summary_text": summary_text, # Th√™m t√≥m t·∫Øt v√†o output
        "script_context_vector_from_summary": script_context_vector, # ƒê·ªïi t√™n ƒë·ªÉ r√µ r√†ng h∆°n
        "script_attributes_mentioned": [] # ƒê·ªÉ tr·ªëng cho vi·ªác g√°n nh√£n th·ªß c√¥ng
    }
    with open(structured_output_filepath, "w", encoding="utf-8") as f:
        json.dump(structured_output, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ D·ªØ li·ªáu script video ƒë√£ l∆∞u ·ªü ƒë·ªãnh d·∫°ng s·∫µn s√†ng g√°n nh√£n: {structured_output_filepath}")


    # üßπ Xo√° file t·∫°m (video v√† audio)
    for file in [video_filename, audio_filename]:
        if os.path.exists(file):
            os.remove(file)
            print(f"üßπ ƒê√£ xo√° file t·∫°m: {file}")

    print(f"\n--- Pipeline x·ª≠ l√Ω video cho {video_id} ho√†n t·∫•t ---")