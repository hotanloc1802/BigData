from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import os
import json

# NgƒÉn Hugging Face t·ª± ƒë·ªông t·∫£i .safetensors
os.environ["HUGGINGFACE_HUB_USE_SAFETENSORS"] = "0"

# --- C·∫•u h√¨nh ---
MODEL_NAME = "VietAI/vit5-large-vietnews-summarization"
INPUT_FOLDER = "../data/script"           # üîß ƒê∆∞·ªùng d·∫´n c·ªë ƒë·ªãnh t·ªõi th∆∞ m·ª•c ch·ª©a c√°c file .txt
OUTPUT_FOLDER = "../data/script/"   # üîß Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£ t√≥m t·∫Øt

# --- T·∫£i m√¥ h√¨nh v√† tokenizer ---
print(f"ƒêang t·∫£i tokenizer v√† m√¥ h√¨nh: {MODEL_NAME}...")
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        MODEL_NAME,
        revision="main",
        trust_remote_code=False,
        low_cpu_mem_usage=True,
        use_safetensors=False
    )
    print("T·∫£i th√†nh c√¥ng.")
except Exception as e:
    print(f"L·ªói khi t·∫£i m√¥ h√¨nh ho·∫∑c tokenizer: {e}")
    exit()

# --- H√†m ƒë·ªçc file .txt ---
def read_script_from_file(file_path: str) -> str:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc file '{file_path}': {e}")
        return ""

# --- H√†m t√≥m t·∫Øt b·∫±ng ViT5 ---
def get_script_context_vit5(script_text: str, max_length: int = 256, min_length: int = 50) -> str:
    if not script_text:
        return "Kh√¥ng c√≥ n·ªôi dung script ƒë·ªÉ t√≥m t·∫Øt."

    input_text = "vietnews: " + script_text + " </s>"
    try:
        input_ids = tokenizer(
            input_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024
        ).input_ids

        summary_ids = model.generate(
            input_ids,
            max_length=max_length,
            min_length=min_length,
            num_beams=5,
            early_stopping=True
        )

        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    except Exception as e:
        return f"L·ªói khi t√≥m t·∫Øt: {e}"

# --- Th·ª±c thi ---
if __name__ == "__main__":
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    txt_files = [f for f in os.listdir(INPUT_FOLDER) if f.endswith(".txt")]

    if not txt_files:
        print(f"Kh√¥ng t√¨m th·∫•y file .txt n√†o trong th∆∞ m·ª•c: {INPUT_FOLDER}")
        exit()

    for txt_file in txt_files:
        full_input_path = os.path.join(INPUT_FOLDER, txt_file)
        print(f"\n--- ƒêang x·ª≠ l√Ω: {txt_file} ---")
        script_content = read_script_from_file(full_input_path)

        if script_content:
            summary = get_script_context_vit5(script_content)
            output_data = {
                "original_script_path": full_input_path,
                "summarized_context": summary
            }

            base_name = os.path.splitext(txt_file)[0]
            output_filename = f"{base_name}_summary.json"
            output_path = os.path.join(OUTPUT_FOLDER, output_filename)

            try:
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(output_data, f, ensure_ascii=False, indent=4)
                print(f"‚úî ƒê√£ l∆∞u: {output_path}")
            except Exception as e:
                print(f"L·ªói khi l∆∞u {output_path}: {e}")
        else:
            print(f"B·ªè qua v√¨ kh√¥ng ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung: {txt_file}")
